---
layout: post
title: "Building AI Literacy at Scale: A CUNY Libraries Experiment"
date: 2026-02-18
author: "Alevtina Verbovetskaya"
excerpt: "Three weeks into a peer mentoring cohort where 13 CUNY librarians are building practical AI tools and developing critical evaluation skills."
tags: [library-systems, technology, leadership, professional-development, artificial-intelligence]
---

Last year, my department began experimenting with new models for systemwide professional development—programs designed to build shared technical capacity across CUNY Libraries rather than deliver one-off trainings. This semester, we’re extending that work in a more hands-on direction.

We’re three weeks into a 16-week peer mentoring cohort exploring agentic AI and its implications for library work.

The initiative is sponsored by the Office of Library Services, where my team focuses on building infrastructure—technical, operational, and, increasingly, human. When Stephen Zweibel, Digital Scholarship Librarian at the Graduate Center, proposed a cohort grounded in practical AI development, I saw an opportunity. Librarians shouldn’t sit on the sidelines while vendors define what AI in libraries looks like. We should understand it well enough to shape it.

Thirteen library faculty and staff from nine CUNY campuses are spending the semester building AI-enabled tools to address real workflow challenges. Stephen is leading the cohort, guiding participants through development while emphasizing critical evaluation: understanding APIs, interrogating model limitations, assessing risk, and distinguishing what’s technically feasible from what’s simply well-marketed.

The project proposals reflect the breadth of challenges libraries face: automating faculty publication tracking for institutional repositories, building accessibility checkers for course content, enhancing discovery systems, streamlining course reserves workflows, and developing CUNY-wide tools for data access. Technical backgrounds range from complete beginners to experienced developers, which makes the peer learning dynamic especially powerful.

This cohort didn’t emerge in a vacuum.

Over the past year, I’ve been integrating code-driven approaches into systemwide infrastructure projects. Our Alma letters repository externalizes patron notification templates—headers, receipts, overdue notices—into version-controlled files, synced across campuses via the Alma Configuration API. Instead of managing 26 slightly different copies in an administrative interface, we define accessible, mobile-responsive templates once and distribute them consistently. We’ve taken a similar approach to standardizing access model descriptions across institution zones, preventing configuration drift where Alma does not provide centralized governance.

Different problems. Same pattern: extract what should be shared, codify it, and apply it reliably at scale.

That experience reinforced something important. The leverage isn’t in the API calls. It’s in librarians deciding what should be standardized, what should remain local, and what “good” looks like across a 26-campus system. AI literacy is a natural extension of that philosophy.

From my side, the work is about scaffolding: structuring the cohort, handling logistics, setting up the shared collaboration space in Microsoft Teams, and creating a framework that supports experimentation without overwhelming participants. It’s administrative work, yes—but it’s also strategic. Capacity building doesn’t happen accidentally. It requires intention, coordination, and sustained support.

Beyond the individual projects, the goal is distributed expertise. Participants will become campus resources—colleagues who can help their institutions navigate vendor claims, evaluate AI tools critically, and understand what’s genuinely possible versus marketing hype. In a 26-campus system, that kind of distributed technical fluency is infrastructure.

The Teams space is already active: participants troubleshooting installation issues together, sharing resources, asking substantive questions about APIs and workflow automation. The commitment level is notable—several participants are balancing significant professional and personal responsibilities to make this work.

What emerges over the next thirteen weeks remains to be seen. I’m watching for what scales, what sustains, and what shifts—not just in the tools being built, but in how we understand technical agency in libraries.

More to come.
